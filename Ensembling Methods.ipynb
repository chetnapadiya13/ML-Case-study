{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhere to learn ensemble we are using banking data where we are try find either person will pay loan or not\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "here to learn ensemble we are using banking data where we are try find either person will pay loan or not\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/bank.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>deposit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2343</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1270</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11157</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11158</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>733</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11159</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11160</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11161</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>628</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11162 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  job  marital  education  default  balance  housing  loan  contact  \\\n",
       "0       59    0        1          1        0     2343        1     0        2   \n",
       "1       56    0        1          1        0       45        0     0        2   \n",
       "2       41    9        1          1        0     1270        1     0        2   \n",
       "3       55    7        1          1        0     2476        1     0        2   \n",
       "4       54    0        1          2        0      184        0     0        2   \n",
       "...    ...  ...      ...        ...      ...      ...      ...   ...      ...   \n",
       "11157   33    1        2          0        0        1        1     0        0   \n",
       "11158   39    7        1          1        0      733        0     0        2   \n",
       "11159   32    9        2          1        0       29        0     0        0   \n",
       "11160   43    9        1          1        0        0        0     1        0   \n",
       "11161   34    9        1          1        0        0        0     0        0   \n",
       "\n",
       "       day  month  duration  campaign  pdays  previous  poutcome  deposit  \n",
       "0        5      8      1042         1     -1         0         3        1  \n",
       "1        5      8      1467         1     -1         0         3        1  \n",
       "2        5      8      1389         1     -1         0         3        1  \n",
       "3        5      8       579         1     -1         0         3        1  \n",
       "4        5      8       673         2     -1         0         3        1  \n",
       "...    ...    ...       ...       ...    ...       ...       ...      ...  \n",
       "11157   20      0       257         1     -1         0         3        0  \n",
       "11158   16      6        83         4     -1         0         3        0  \n",
       "11159   19      1       156         2     -1         0         3        0  \n",
       "11160    8      8         9         2    172         5         0        0  \n",
       "11161    9      5       628         1     -1         0         3        0  \n",
       "\n",
       "[11162 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11162 entries, 0 to 11161\n",
      "Data columns (total 17 columns):\n",
      "age          11162 non-null int64\n",
      "job          11162 non-null int64\n",
      "marital      11162 non-null int64\n",
      "education    11162 non-null int64\n",
      "default      11162 non-null int64\n",
      "balance      11162 non-null int64\n",
      "housing      11162 non-null int64\n",
      "loan         11162 non-null int64\n",
      "contact      11162 non-null int64\n",
      "day          11162 non-null int64\n",
      "month        11162 non-null int64\n",
      "duration     11162 non-null int64\n",
      "campaign     11162 non-null int64\n",
      "pdays        11162 non-null int64\n",
      "previous     11162 non-null int64\n",
      "poutcome     11162 non-null int64\n",
      "deposit      11162 non-null int64\n",
      "dtypes: int64(17)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5873\n",
       "1    5289\n",
       "Name: deposit, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"deposit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "df[\"deposit\"].value_counts().plot.pie(autopct=\"%1.1f%%\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2343</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1270</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11157</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>257</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11158</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>733</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11159</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11160</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11161</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>628</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11162 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  job  marital  education  default  balance  housing  loan  contact  \\\n",
       "0       59    0        1          1        0     2343        1     0        2   \n",
       "1       56    0        1          1        0       45        0     0        2   \n",
       "2       41    9        1          1        0     1270        1     0        2   \n",
       "3       55    7        1          1        0     2476        1     0        2   \n",
       "4       54    0        1          2        0      184        0     0        2   \n",
       "...    ...  ...      ...        ...      ...      ...      ...   ...      ...   \n",
       "11157   33    1        2          0        0        1        1     0        0   \n",
       "11158   39    7        1          1        0      733        0     0        2   \n",
       "11159   32    9        2          1        0       29        0     0        0   \n",
       "11160   43    9        1          1        0        0        0     1        0   \n",
       "11161   34    9        1          1        0        0        0     0        0   \n",
       "\n",
       "       day  month  duration  campaign  pdays  previous  poutcome  \n",
       "0        5      8      1042         1     -1         0         3  \n",
       "1        5      8      1467         1     -1         0         3  \n",
       "2        5      8      1389         1     -1         0         3  \n",
       "3        5      8       579         1     -1         0         3  \n",
       "4        5      8       673         2     -1         0         3  \n",
       "...    ...    ...       ...       ...    ...       ...       ...  \n",
       "11157   20      0       257         1     -1         0         3  \n",
       "11158   16      6        83         4     -1         0         3  \n",
       "11159   19      1       156         2     -1         0         3  \n",
       "11160    8      8         9         2    172         5         0  \n",
       "11161    9      5       628         1     -1         0         3  \n",
       "\n",
       "[11162 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "11157    0\n",
       "11158    0\n",
       "11159    0\n",
       "11160    0\n",
       "11161    0\n",
       "Name: deposit, Length: 11162, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()\n",
    "dt1=DecisionTreeClassifier() # gini index 1-p^2-q^2\n",
    "dt2=DecisionTreeClassifier(criterion=\"entropy\") # entropy= 1- log(p)^2 -log(q)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=[('lr',lr),('dt1',dt1),('dt2',dt2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by default votingclassifier go with hard voting \n",
    "hvc=VotingClassifier(estimators=model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('dt1',\n",
       "                              DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini'...\n",
       "                              DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='entropy',\n",
       "                                                     max_depth=None,\n",
       "                                                     max_features=None,\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     presort='deprecated',\n",
       "                                                     random_state=None,\n",
       "                                                     splitter='best'))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hvc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=hvc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83      1760\n",
      "           1       0.81      0.81      0.81      1589\n",
      "\n",
      "    accuracy                           0.82      3349\n",
      "   macro avg       0.82      0.82      0.82      3349\n",
      "weighted avg       0.82      0.82      0.82      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=VotingClassifier(estimators=model_list,voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('dt1',\n",
       "                              DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini'...\n",
       "                              DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='entropy',\n",
       "                                                     max_depth=None,\n",
       "                                                     max_features=None,\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     presort='deprecated',\n",
       "                                                     random_state=None,\n",
       "                                                     splitter='best'))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83      1760\n",
      "           1       0.81      0.82      0.81      1589\n",
      "\n",
      "    accuracy                           0.82      3349\n",
      "   macro avg       0.82      0.82      0.82      3349\n",
      "weighted avg       0.82      0.82      0.82      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BootStrap Aggregation (1- Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn_estimators=10 means 10 lr objects\\nmax_samples=100 it's creat 100 random sample to train our model \\nrandom_state=1 means similar type of sampling will happen in each machine execution \\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg=BaggingClassifier(LogisticRegression(), n_estimators=10, max_samples=100,random_state=1)\n",
    "'''\n",
    "n_estimators=10 means 10 lr objects\n",
    "max_samples=100 it's creat 100 random sample to train our model \n",
    "random_state=1 means similar type of sampling will happen in each machine execution \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2506</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>562</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>494</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2827</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9197</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3975</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9910</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6507</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7813</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>744</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>609</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10955</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1451</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5192</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>985</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>274</td>\n",
       "      <td>2</td>\n",
       "      <td>391</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>403</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>920</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7813 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  job  marital  education  default  balance  housing  loan  contact  \\\n",
       "2506    74    5        1          0        0        0        0     0        1   \n",
       "368     55    1        1          1        0       49        1     0        0   \n",
       "2827    57    9        1          1        0      254        0     0        0   \n",
       "9197    37    0        0          1        0     3975        1     0        0   \n",
       "9910    56    4        2          2        0     6507        1     1        0   \n",
       "...    ...  ...      ...        ...      ...      ...      ...   ...      ...   \n",
       "7813    26    9        1          1        0      744        0     0        0   \n",
       "10955   39    9        1          1        0    -1451        1     1        0   \n",
       "905     27    2        1          1        0     2329        0     0        1   \n",
       "5192    41    0        2          1        0      985        1     0        0   \n",
       "235     36    1        1          0        0      403        1     0        2   \n",
       "\n",
       "       day  month  duration  campaign  pdays  previous  poutcome  \n",
       "2506    13      1       562         2     -1         0         3  \n",
       "368      7      5       494         4     -1         0         3  \n",
       "2827    27     10       207         1     -1         0         3  \n",
       "9197    11      8       163         2     -1         0         3  \n",
       "9910    29      4       134         1    196         1         0  \n",
       "...    ...    ...       ...       ...    ...       ...       ...  \n",
       "7813    15      0       609         1     -1         0         3  \n",
       "10955   30      5        65         5     -1         0         3  \n",
       "905     27     10       131         1     -1         0         3  \n",
       "5192    30      6       274         2    391        11         2  \n",
       "235      4      6       920         1     -1         0         3  \n",
       "\n",
       "[7813 rows x 16 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2506     1\n",
       "368      1\n",
       "2827     1\n",
       "9197     0\n",
       "9910     0\n",
       "        ..\n",
       "7813     0\n",
       "10955    0\n",
       "905      1\n",
       "5192     1\n",
       "235      1\n",
       "Name: deposit, Length: 7813, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                    dual=False,\n",
       "                                                    fit_intercept=True,\n",
       "                                                    intercept_scaling=1,\n",
       "                                                    l1_ratio=None, max_iter=100,\n",
       "                                                    multi_class='auto',\n",
       "                                                    n_jobs=None, penalty='l2',\n",
       "                                                    random_state=None,\n",
       "                                                    solver='lbfgs', tol=0.0001,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=100, n_estimators=10, n_jobs=None,\n",
       "                  oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=bg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.82      0.79      1760\n",
      "           1       0.78      0.70      0.74      1589\n",
      "\n",
      "    accuracy                           0.77      3349\n",
      "   macro avg       0.77      0.76      0.77      3349\n",
      "weighted avg       0.77      0.77      0.77      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwhen we are aware of data patterns which is good for logistic regression only we can go for \\nbootstrap aggrigation \\n\\nbut if data pattern is very complex and one ml algorith is not efficient enough to give \\nbetter accuracy go for stacking or naive aggrigation \\n\\nbagging can be possible with replacement of data and without replacement of data \\nif i am performing bagging without replacement it is known as pasting \\nother process of bagging will be same \\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "when we are aware of data patterns which is good for logistic regression only we can go for \n",
    "bootstrap aggrigation \n",
    "\n",
    "but if data pattern is very complex and one ml algorith is not efficient enough to give \n",
    "better accuracy go for stacking or naive aggrigation \n",
    "\n",
    "bagging can be possible with replacement of data and without replacement of data \n",
    "if i am performing bagging without replacement it is known as pasting \n",
    "other process of bagging will be same \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BootStrap Aggregation (2- Pasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bootstrap=False means no replacement or repeation in each sample of data \n",
    "bg2 = BaggingClassifier(LogisticRegression(),n_estimators=10,max_samples=100,random_state=1,bootstrap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                    dual=False,\n",
       "                                                    fit_intercept=True,\n",
       "                                                    intercept_scaling=1,\n",
       "                                                    l1_ratio=None, max_iter=100,\n",
       "                                                    multi_class='auto',\n",
       "                                                    n_jobs=None, penalty='l2',\n",
       "                                                    random_state=None,\n",
       "                                                    solver='lbfgs', tol=0.0001,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                  bootstrap=False, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=100, n_estimators=10, n_jobs=None,\n",
       "                  oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg2.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                    dual=False,\n",
       "                                                    fit_intercept=True,\n",
       "                                                    intercept_scaling=1,\n",
       "                                                    l1_ratio=None, max_iter=100,\n",
       "                                                    multi_class='auto',\n",
       "                                                    n_jobs=None, penalty='l2',\n",
       "                                                    random_state=None,\n",
       "                                                    solver='lbfgs', tol=0.0001,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                  bootstrap=False, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=100, n_estimators=10, n_jobs=None,\n",
       "                  oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg2.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=bg2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.79      1760\n",
      "           1       0.78      0.71      0.74      1589\n",
      "\n",
      "    accuracy                           0.77      3349\n",
      "   macro avg       0.77      0.76      0.77      3349\n",
      "weighted avg       0.77      0.77      0.77      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\neither we take bagging or pasting no change in 10 estomator based ensemble model training \\nso no need to go with pasting bagging is enough \\nwhen my data have high bias at that time i can go with bagging \\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "either we take bagging or pasting no change in 10 estomator based ensemble model training \n",
    "so no need to go with pasting bagging is enough \n",
    "when my data have high bias at that time i can go with bagging \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # it is ensemble techniq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwhen our decision tree is not giving proper classification and my feature have \\nvarity of data at that time it is necessary we go feature by feature to classify our target \\nfor that we need more than 1 decision teach each feature have it's own decision tree\\nso the combination of multiple tree and it's classification known as random forest \\nit is complex to understand as compare to decision tree but \\nit reduces overfit problem it can be implemented when we have less amount of data \\nas it required more memory and more processing time when we have more features at that time also\\nit is not suggested \\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "when our decision tree is not giving proper classification and my feature have \n",
    "varity of data at that time it is necessary we go feature by feature to classify our target \n",
    "for that we need more than 1 decision teach each feature have it's own decision tree\n",
    "so the combination of multiple tree and it's classification known as random forest \n",
    "it is complex to understand as compare to decision tree but \n",
    "it reduces overfit problem it can be implemented when we have less amount of data \n",
    "as it required more memory and more processing time when we have more features at that time also\n",
    "it is not suggested \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(n_estimators=10,max_features=16,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7813 entries, 2506 to 235\n",
      "Data columns (total 16 columns):\n",
      "age          7813 non-null int64\n",
      "job          7813 non-null int64\n",
      "marital      7813 non-null int64\n",
      "education    7813 non-null int64\n",
      "default      7813 non-null int64\n",
      "balance      7813 non-null int64\n",
      "housing      7813 non-null int64\n",
      "loan         7813 non-null int64\n",
      "contact      7813 non-null int64\n",
      "day          7813 non-null int64\n",
      "month        7813 non-null int64\n",
      "duration     7813 non-null int64\n",
      "campaign     7813 non-null int64\n",
      "pdays        7813 non-null int64\n",
      "previous     7813 non-null int64\n",
      "poutcome     7813 non-null int64\n",
      "dtypes: int64(16)\n",
      "memory usage: 1.0 MB\n"
     ]
    }
   ],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features=16,\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(x_train,y_train) #boostrap=True means repeation of data or replacement of data is allowed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83      1760\n",
      "           1       0.81      0.83      0.82      1589\n",
      "\n",
      "    accuracy                           0.83      3349\n",
      "   macro avg       0.83      0.83      0.83      3349\n",
      "weighted avg       0.83      0.83      0.83      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as per naive aggregation 2% accuracy increased \n",
    "# random forest find importenet features as well as perform missing value treatment also "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nit is hetro geneous we have seen naive agreggation as an example of stacking \\nwhere we use votingclassifier and tried to understand soft voting and hard voting \\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "it is hetro geneous we have seen naive agreggation as an example of stacking \n",
    "where we use votingclassifier and tried to understand soft voting and hard voting \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\programdata\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (0.22.2.post1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (41.4.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (0.13.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (3.1.3)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.19.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from mlxtend) (1.4.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2019.3)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# another library to perform stacking with advance feature \n",
    "! pip install mlxtend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nas we know process of stacking where it uses multiple ml algorith and \\ngive prediction and those predicted target will be use again as a feature and develop \\nmeta model here also we took base model \\nlr,dt gigi,dt entropy \\nmeta model is lr again \\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "as we know process of stacking where it uses multiple ml algorith and \n",
    "give prediction and those predicted target will be use again as a feature and develop \n",
    "meta model here also we took base model \n",
    "lr,dt gigi,dt entropy \n",
    "meta model is lr again \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()\n",
    "dt1=DecisionTreeClassifier()\n",
    "dt2=DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model_list=[lr,dt1,dt2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_lr=LogisticRegression()\n",
    "meta_dt=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "stack=StackingClassifier(classifiers=model_list, meta_classifier=meta_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(average_probas=False,\n",
       "                   classifiers=[LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                   dual=False,\n",
       "                                                   fit_intercept=True,\n",
       "                                                   intercept_scaling=1,\n",
       "                                                   l1_ratio=None, max_iter=100,\n",
       "                                                   multi_class='auto',\n",
       "                                                   n_jobs=None, penalty='l2',\n",
       "                                                   random_state=None,\n",
       "                                                   solver='lbfgs', tol=0.0001,\n",
       "                                                   verbose=0,\n",
       "                                                   warm_start=False),\n",
       "                                DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                       class_weight=None,\n",
       "                                                       criter...\n",
       "                                                          criterion='gini',\n",
       "                                                          max_depth=None,\n",
       "                                                          max_features=None,\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          presort='deprecated',\n",
       "                                                          random_state=None,\n",
       "                                                          splitter='best'),\n",
       "                   store_train_meta_features=False, use_clones=True,\n",
       "                   use_features_in_secondary=False, use_probas=False,\n",
       "                   verbose=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=stack.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      1760\n",
      "           1       0.78      0.77      0.78      1589\n",
      "\n",
      "    accuracy                           0.79      3349\n",
      "   macro avg       0.79      0.79      0.79      3349\n",
      "weighted avg       0.79      0.79      0.79      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c anaconda py-xgboost\n",
    "# ! pip3 install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84      1760\n",
      "           1       0.83      0.80      0.81      1589\n",
      "\n",
      "    accuracy                           0.83      3349\n",
      "   macro avg       0.83      0.83      0.83      3349\n",
      "weighted avg       0.83      0.83      0.83      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada=AdaBoostClassifier()\n",
    "ada.fit(x_train,y_train)\n",
    "y_pred=ada.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Gradientboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83      1760\n",
      "           1       0.81      0.83      0.82      1589\n",
      "\n",
      "    accuracy                           0.83      3349\n",
      "   macro avg       0.83      0.83      0.83      3349\n",
      "weighted avg       0.83      0.83      0.83      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#if i will not specify parameters then it will be greedy by nature so we decide to keep limited feature and max_depth\n",
    "gb = GradientBoostingClassifier(n_estimators=20,\n",
    "                                learning_rate = 0.5,\n",
    "                                max_features=10,\n",
    "                                max_depth = 2,\n",
    "                                random_state = 0)\n",
    "gb.fit(x_train,y_train)\n",
    "y_pred = gb.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.05\n",
      "Accuracy score (training): 0.735\n",
      "Accuracy score (testing): 0.715\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 0.768\n",
      "Accuracy score (testing): 0.752\n",
      "Learning rate:  0.25\n",
      "Accuracy score (training): 0.808\n",
      "Accuracy score (testing): 0.796\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 0.826\n",
      "Accuracy score (testing): 0.815\n",
      "Learning rate:  0.75\n",
      "Accuracy score (training): 0.825\n",
      "Accuracy score (testing): 0.819\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 0.828\n",
      "Accuracy score (testing): 0.821\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "for i in learning_rates:\n",
    "    gb = GradientBoostingClassifier(n_estimators=20,\n",
    "                                    learning_rate = i,\n",
    "                                    max_features=2,\n",
    "                                    max_depth = 2,\n",
    "                                    random_state = 1)\n",
    "    gb.fit(x_train,y_train)\n",
    "    print(\"Learning rate: \",i)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(x_train, y_train)))\n",
    "    print(\"Accuracy score (testing): {0:.3f}\".format(gb.score(x_test, y_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83      1760\n",
      "           1       0.81      0.82      0.81      1589\n",
      "\n",
      "    accuracy                           0.82      3349\n",
      "   macro avg       0.82      0.82      0.82      3349\n",
      "weighted avg       0.82      0.82      0.82      3349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For better results we need to  tune the parameters\n",
    "from xgboost import XGBClassifier\n",
    "xgboost = XGBClassifier(learning_rate =0.05,\n",
    " n_estimators=500,\n",
    " max_depth=2,\n",
    " min_child_weight=1)\n",
    "xgboost.fit(x_train, y_train)\n",
    "y_pred = gb.predict(x_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allmost same to random forest "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
